packages <- c("mlbench", "caret", "randomForest", "DALEX", "ggplot2")
install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)
library(mlbench)
library(caret)
library(randomForest)
library(DALEX)
library(ggplot2)
#Load the Pima Indians Diabetes Dataset
data(PimaIndiansDiabetes)
df <- na.omit(PimaIndiansDiabetes)
cat("Dataset loaded successfully!\n")
head(df)
#Split Data into Training and Testing Sets
set.seed(123)
index <- createDataPartition(df$diabetes, p = 0.8, list = FALSE)
train <- df[index, ]
test  <- df[-index, ]
cat("Training and test sets created.\n")
#Train the Random Forest Model
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 100, importance = TRUE)
print(model_rf)
#Evaluate Model Performance
pred <- predict(model_rf, test)
conf_mat <- confusionMatrix(pred, test$diabetes)
cat("\nConfusion Matrix:\n")
print(conf_mat)
#Feature Importance
cat("\nFeature Importance (Random Forest):\n")
print(importance(model_rf))
# Plot feature importance
varImpPlot(model_rf, main = "Feature Importance in Diabetes Prediction")
#Explain the Model using DALEX
explainer_rf <- explain(model_rf,
data = test[, -9],
y = test$diabetes,
label = "Random Forest")
vip <- model_parts(explainer_rf)
#Save Model and Session Info
saveRDS(model_rf, "diabetes_rf_model.rds")
cat("\nModel saved as diabetes_rf_model.rds\n")
cat("\nSession Information:\n")
print(sessionInfo())
library(mlbench)
library(caret)
library(randomForest)
library(DALEX)
library(ggplot2)
#Load the Pima Indians Diabetes Dataset
data(PimaIndiansDiabetes)
df <- na.omit(PimaIndiansDiabetes)
cat("Dataset loaded successfully!\n")
head(df)
#Split Data into Training and Testing Sets
set.seed(123)
index <- createDataPartition(df$diabetes, p = 0.8, list = FALSE)
train <- df[index, ]
test  <- df[-index, ]
cat("Training and test sets created.\n")
#Train the Random Forest Model
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 100, importance = TRUE)
print(model_rf)
#Evaluate Model Performance
pred <- predict(model_rf, test)
conf_mat <- confusionMatrix(pred, test$diabetes)
cat("\nConfusion Matrix:\n")
print(conf_mat)
#Feature Importance
cat("\nFeature Importance (Random Forest):\n")
print(importance(model_rf))
# Plot feature importance
varImpPlot(model_rf, main = "Feature Importance in Diabetes Prediction")
#Explain the Model using DALEX
explainer_rf <- explain(model_rf,
data = test[, -9],
y = test$diabetes,
label = "Random Forest")
vip <- model_parts(explainer_rf)
#Explain the Model using DALEX
explainer_rf <- explain(model_rf,
data = test[, -9],
y = test$diabetes,
label = "Random Forest")
vip <- model_parts(explainer_rf)
#Bias and Fairness Check
cat("\nClass distribution in dataset:\n")
print(table(df$diabetes))
#Save Model and Session Info
saveRDS(model_rf, "diabetes_rf_model.rds")
cat("\nModel saved as diabetes_rf_model.rds\n")
cat("\nSession Information:\n")
print(sessionInfo())
library(mlbench)
library(caret)
library(randomForest)
library(DALEX)
library(ggplot2)
#Load the Pima Indians Diabetes Dataset
data(PimaIndiansDiabetes)
df <- na.omit(PimaIndiansDiabetes)
cat("Dataset loaded successfully!\n")
head(df)
#Split Data into Training and Testing Sets
set.seed(123)
index <- createDataPartition(df$diabetes, p = 0.8, list = FALSE)
train <- df[index, ]
test  <- df[-index, ]
cat("Training and test sets created.\n")
#Train the Random Forest Model
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 100, importance = TRUE)
print(model_rf)
#Evaluate Model Performance
pred <- predict(model_rf, test)
conf_mat <- confusionMatrix(pred, test$diabetes)
cat("\nConfusion Matrix:\n")
print(conf_mat)
#Feature Importance
cat("\nFeature Importance (Random Forest):\n")
print(importance(model_rf))
# Plot feature importance
varImpPlot(model_rf, main = "Feature Importance in Diabetes Prediction")
#Explain the Model using DALEX
explainer_rf <- explain(model_rf,
data = test[, -9],
y = test$diabetes,
label = "Random Forest")
vip <- model_parts(explainer_rf)
library(mlbench)
library(caret)
library(randomForest)
library(DALEX)
library(ggplot2)
#Load the Pima Indians Diabetes Dataset
data(PimaIndiansDiabetes)
df <- na.omit(PimaIndiansDiabetes)
cat("Dataset loaded successfully!\n")
head(df)
#Split Data into Training and Testing Sets
set.seed(123)
index <- createDataPartition(df$diabetes, p = 0.8, list = FALSE)
train <- df[index, ]
test  <- df[-index, ]
cat("Training and test sets created.\n")
#Train the Random Forest Model
model_rf <- randomForest(diabetes ~ ., data = train, ntree = 100, importance = TRUE)
print(model_rf)
#Evaluate Model Performance
pred <- predict(model_rf, test)
conf_mat <- confusionMatrix(pred, test$diabetes)
cat("\nConfusion Matrix:\n")
print(conf_mat)
#Feature Importance
cat("\nFeature Importance (Random Forest):\n")
print(importance(model_rf))
# Plot feature importance
varImpPlot(model_rf, main = "Feature Importance in Diabetes Prediction")
#Explain the Model using DALEX
explainer_rf <- explain(model_rf,
data = test[, -9],
y = test$diabetes,
label = "Random Forest")
vip <- model_parts(explainer_rf)
#Bias and Fairness Check
cat("\nClass distribution in dataset:\n")
print(table(df$diabetes))
#Save Model and Session Info
saveRDS(model_rf, "diabetes_rf_model.rds")
cat("\nModel saved as diabetes_rf_model.rds\n")
cat("\nSession Information:\n")
print(sessionInfo())
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(dplyr)
library(ggplot2)
library(GGally)
library(dplyr)
library(ggplot2)
#library(GGally)
library(ggcorrplot)
library(dplyr)
library(ggplot2)
#library(GGally)
library(corrplot)
# Load dataset
data("iris")
head(iris)
# 1. Summary statistics
summary(iris)
# 2. Histogram of Sepal.Length
ggplot(iris, aes(x = Sepal.Length)) +
geom_histogram(bins = 20, fill = "lightblue", color = "black") +
labs(title = "Distribution of Sepal Length", x = "Sepal Length", y = "Frequency")
# 3. Scatterplot Sepal.Length vs Petal.Length
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point(size = 3) +
labs(title = "Sepal Length vs Petal Length")
# 4. Boxplot of Sepal.Width by Species
ggplot(iris, aes(x = Species, y = Sepal.Width, fill = Species)) +
geom_boxplot() +
labs(title = "Boxplot of Sepal Width by Species")
# 5. Correlation matrix
corr_matrix <- cor(iris[, 1:4])
corr_matrix
ggcorrplot(corr_matrix, lab = TRUE, title = "Correlation Matrix Heatmap")
library(dplyr)
library(ggplot2)
#library(GGally)
library(corrplot)
# Load dataset
data("iris")
head(iris)
# 1. Summary statistics
summary(iris)
# 2. Histogram of Sepal.Length
ggplot(iris, aes(x = Sepal.Length)) +
geom_histogram(bins = 20, fill = "lightblue", color = "black") +
labs(title = "Distribution of Sepal Length", x = "Sepal Length", y = "Frequency")
# 3. Scatterplot Sepal.Length vs Petal.Length
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point(size = 3) +
labs(title = "Sepal Length vs Petal Length")
# 4. Boxplot of Sepal.Width by Species
ggplot(iris, aes(x = Species, y = Sepal.Width, fill = Species)) +
geom_boxplot() +
labs(title = "Boxplot of Sepal Width by Species")
# 5. Correlation matrix
corr_matrix <- cor(iris[, 1:4])
corr_matrix
corrplot(corr_matrix, lab = TRUE, title = "Correlation Matrix Heatmap")
library(dplyr)
library(ggplot2)
#library(GGally)
library(corrplot)
# Load dataset
data("iris")
head(iris)
# 1. Summary statistics
summary(iris)
# 2. Histogram of Sepal.Length
ggplot(iris, aes(x = Sepal.Length)) +
geom_histogram(bins = 20, fill = "lightblue", color = "black") +
labs(title = "Distribution of Sepal Length", x = "Sepal Length", y = "Frequency")
# 3. Scatterplot Sepal.Length vs Petal.Length
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point(size = 3) +
labs(title = "Sepal Length vs Petal Length")
# 4. Boxplot of Sepal.Width by Species
ggplot(iris, aes(x = Species, y = Sepal.Width, fill = Species)) +
geom_boxplot() +
labs(title = "Boxplot of Sepal Width by Species")
# 5. Correlation matrix
corr_matrix <- cor(iris[, 1:4])
corr_matrix
corrplot(corr_matrix,
method = "color",
addCoef.col = "black",   # show numbers
number.cex = 0.7,        # number size
tl.col = "black",        # text label color
tl.srt = 45,             # x-axis label rotation
main = "Correlation Matrix Heatmap")
# 6. Pair plot
pairs(iris[, 1:4])
# Load libraries
library(dplyr)
library(ggplot2)
library(corrplot)
# Load dataset
data("iris")
head(iris)
# 1. Summary statistics
summary(iris)
# 2. Histogram of Sepal.Length
ggplot(iris, aes(x = Sepal.Length)) +
geom_histogram(bins = 20, fill = "lightblue", color = "black") +
labs(title = "Distribution of Sepal Length", x = "Sepal Length", y = "Frequency")
# 3. Scatterplot: Sepal.Length vs Petal.Length
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point(size = 3) +
labs(title = "Sepal Length vs Petal Length")
# 4. Boxplot: Sepal.Width by Species
ggplot(iris, aes(x = Species, y = Sepal.Width, fill = Species)) +
geom_boxplot() +
labs(title = "Boxplot of Sepal Width by Species")
# 5. Correlation matrix heatmap (corrplot)
corr_matrix <- cor(iris[, 1:4])
corrplot(corr_matrix,
method = "color",
addCoef.col = "black",   # add correlation numbers
number.cex = 0.7,
tl.col = "black",
tl.srt = 45,
main = "Correlation Matrix Heatmap")
# 6. Pair plot (base R alternative to GGally::ggpairs)
pairs(iris[, 1:4], main = "Pair Plot of Iris Dataset")
library(dplyr)
# Load iris dataset
data("iris")
# 1. t-test: Compare Sepal.Length of setosa and versicolor
t_test_result <- t.test(Sepal.Length ~ Species,
data = iris %>%
filter(Species %in% c("setosa", "versicolor")))
t_test_result
# 2. ANOVA: Compare Sepal.Length across all species
anova_model <- aov(Sepal.Length ~ Species, data = iris)
summary(anova_model)
# 3. Correlation: Sepal.Length and Petal.Length
correlation <- cor(iris$Sepal.Length, iris$Petal.Length)
correlation
# 4. Correlation test with significance
cor_test <- cor.test(iris$Sepal.Length, iris$Petal.Length)
cor_test
library(dplyr)
library(ggplot2)
# Load dataset
data("mtcars")
head(mtcars)
# 1. Simple Linear Regression: mpg predicted by wt
model_simple <- lm(mpg ~ wt, data = mtcars)
summary(model_simple)
# 2. Plot regression line
ggplot(mtcars, aes(x = wt, y = mpg)) +
geom_point(color = "blue") +
geom_smooth(method = "lm", se = TRUE, color = "red") +
labs(title = "Simple Linear Regression: MPG vs Weight",
x = "Weight (1000 lbs)",
y = "Miles per Gallon")
# 3. Multiple Linear Regression: mpg predicted by wt and hp
model_multiple <- lm(mpg ~ wt + hp, data = mtcars)
summary(model_multiple)
# 4. Residual diagnostics
plot(model_multiple, which = 1)   # Residuals vs Fitted
plot(model_multiple, which = 2)   # Q-Q Plot
library(dplyr)
library(ggplot2)
# Load dataset
data("mtcars")
head(mtcars)
# 1. Simple Linear Regression: mpg predicted by wt
model_simple <- lm(mpg ~ wt, data = mtcars)
summary(model_simple)
# 2. Plot regression line
ggplot(mtcars, aes(x = wt, y = mpg)) +
geom_point(color = "blue") +
geom_smooth(method = "lm", se = TRUE, color = "red") +
labs(title = "Simple Linear Regression: MPG vs Weight",
x = "Weight (1000 lbs)",
y = "Miles per Gallon")
# 3. Multiple Linear Regression: mpg predicted by wt and hp
model_multiple <- lm(mpg ~ wt + hp, data = mtcars)
summary(model_multiple)
# 4. Residual diagnostics
plot(model_multiple, which = 1)   # Residuals vs Fitted
plot(model_multiple, which = 2)   # Q-Q Plot
